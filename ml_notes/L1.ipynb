{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55845a21",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c16c3d",
   "metadata": {},
   "source": [
    "## Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e96c88",
   "metadata": {},
   "source": [
    "### What is Machine Learning\n",
    "- **Machine Learning:** machine learning is a field of study that gives computers the ability to learn from data without being explicitly program (no explicit specification on data pattern)\n",
    "- Three pillars of ML:\n",
    "    - Supervised Learning\n",
    "    - Unsupervised Learning\n",
    "    - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f832b3e",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "- Learning input-output mapping based on a dataset with features and labelled output\n",
    "- Example: using linear regression model to predict outcomes\n",
    "- Diff between ML and Econometrics:\n",
    "    - Econometrics:\n",
    "        - Econometrics build models based on mathmatics and assumptions\n",
    "        - This resulsts in good interpretability, exact anticipation on model capability, but weak prediction\n",
    "    - Machine learning:\n",
    "        - ML builds models from engineering perspective\n",
    "        - Find best model from trails\n",
    "        - Without exact understanding on why the model is capable on specific tasks\n",
    "        - Weak  model interpretability\n",
    "- Regression and classification:\n",
    "    - **Regression:** the outcome of prediction is continous\n",
    "    - **Classification:** the outcome of prediction is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa66be",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "- Unsupervised learning applies to datasets with input features but no labeled outputs\n",
    "- This means that unsupervised learning can only learn distribution of the input data (for tasks such as clustering)\n",
    "- The learned distribution can then be used for generation taks (generative unsupervised models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29345bb5",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "- The model interacts with an environment\n",
    "- It observes the state of the environment (as input)\n",
    "- It learns the optimal action policy through interaction with the environment:\n",
    "    - Observes the state of the environment\n",
    "    - Choose an action according to a policy\n",
    "    - Execute the action\n",
    "    - Observe the reward and change in state\n",
    "    - Update the policy based on the reward signal\n",
    "    - Iterate theough above process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3d9c6",
   "metadata": {},
   "source": [
    "## Probability Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad299dd9",
   "metadata": {},
   "source": [
    "### Joint and Marginal Distributions\n",
    "- Joint distribution: \n",
    "    - A function that maps a vectors to the probability of relaization of the vector\n",
    "    - $Pr(x, y)$\n",
    "- Marginal distribution: \n",
    "    - Captures the probability of realization of one variable regardless of the value the other one took\n",
    "    - Marginalization:\n",
    "        - If we know the joint distribution P(x, y) over two variables, we can recover the marginal distributions\n",
    "        - $Pr(y) = \\int \\Pr(x, y) dx$\n",
    "        - $Pr(x) = \\int \\Pr(x, y) dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e3b96",
   "metadata": {},
   "source": [
    "### Conditional Distribution\n",
    "- Conditional probability $Pr(x|y)$ is the probability of variable x taking a certain value, given the value of y\n",
    "- $$Pr(x|y) = \\frac{Pr(x, y)}{Pr(y)} = \\frac{Pr(x, y)}{\\int \\Pr(x, y) dx}$$\n",
    "- Intutition:\n",
    "    - The conditional distribution $Pr(x|y)$ can be found by taking a slice through the joint distribution $Pr(x, y) for a fixed y\n",
    "    - This slice is then divided by the probability of that value y occurring (the total area under the slice)\n",
    "    - So that the conditional distribution sums to one \n",
    "- Bayesâ€™ rule: \n",
    "    - $$ Pr(x|y) = \\frac{Pr(x, y)}{Pr(y)} = \\frac{Pr(y|x)Pr(x)}{Pr(y)}$$\n",
    "    - $$ Pr(x|y)Pr(y) = Pr(y|x)Pr(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3a346",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Bivariate Gaussian\n",
    "- The joint distribution function: $$\n",
    "P(x; \\mu, \\Sigma) \\propto \n",
    "\\exp\\!\\left[\n",
    "-\\frac{1}{2} (x - \\mu)^{T} \\Sigma^{-1} (x - \\mu)\n",
    "\\right] $$\n",
    "- The distribution is characterized by $x$ (a vector of quantity ), $\\mu$ (mean vector), $\\Sigma$ (covariance matrix)\n",
    "- The covariance matrix can take spherical (multiple of the identity matrix), diagonal, and full forms (not diagonal):\n",
    "    - Spherical: isocontours (contours) of the distribution are circles\n",
    "    - Diagonal: isocontours (contours) of the distribution are axis-aligned ellipses\n",
    "    - Full: isocontours (contours) are general ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ef2eb",
   "metadata": {},
   "source": [
    "## Information Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588b267",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence\n",
    "- Usually referred as \"distance\" between probability distributions $p(x)$ and $q(x)$\n",
    "- It tells how similar two distribution are, however, it is not distance as the measure is not symmetric (KL divergence of Q from P is not the same as P from Q)\n",
    "- KL divergence of Q from P (or relative entropy of P w.r.t to Q):\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P \\,\\|\\, Q)\n",
    "=\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "p(x)\\,\n",
    "\\log\\!\\left(\n",
    "\\frac{p(x)}{q(x)}\n",
    "\\right)\n",
    "\\, dx\n",
    "$$\n",
    "- Properties:\n",
    "    - If Q is exactly the same as P: the likelihood ratio is 1, the log-likelihood ratio is 0, the integral is 0, therefore, the KL divergence is 0\n",
    "    - If Q is quite different from P: the log-likelihood ratio is either >0 or <0, the divergence measure > 0\n",
    "- In Bayesian inference for instance, it can measure information gain from moving from prior Q to posterior P (if Q quite far from P, high information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedefbb",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "- Shannon entropy is a measure of uncertainty of a distribution (related to its variance)\n",
    "- Intuition:\n",
    "    - How many bits, on average, are needed to remove uncertainty from a distribution?\n",
    "    - Equivalently, what is the minimum number of yes/no questions needed to determine what a person is thinking (x)?\n",
    "    - Suppose x is drawn from a known distribution, the entropy measures how many questions we need to ask to determine x\n",
    "    - For example, if x is drawn from a Bernoulli distribution, we only need to ask one question: is it 1?\n",
    "- Definition:\n",
    "    - In discrete case : \n",
    "    $$ \n",
    "    H(X)\n",
    "    = - \\sum_x p(x)\\,\\log p(x)\n",
    "    $$\n",
    "    - In continous case (differential entropy): \n",
    "    $$ \n",
    "    h(X)\n",
    "    = - \\int_{-\\infty}^{\\infty} p(x)\\,\\log p(x)\\, dx\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dee3a0",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "- For two random variable X and Y, mutual information is the KL divergence of product of marginals of X and Y from joint distribution of (X,Y)\n",
    "- It measures how much one random variable tells us about another, how informative X is about Y\n",
    "- It is related to correlation, but correlation captures only linear dependence, while mutual information is a generalization (0 if independent)\n",
    "- Intuition:\n",
    "    - It is the expected reduction in entropy of X given value of Y\n",
    "    - If the value of Y is given, how much uncertainty is reduced for the distribution of X\n",
    "    - If the value of Y is given, how many minimum yes/no questions can be eliminated when guessing x\n",
    "- Definition: \n",
    "$$\n",
    "I(X; Y)\n",
    "=\n",
    "\\iint\n",
    "p_{X,Y}(x,y)\n",
    "\\log\\!\\left(\n",
    "\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\n",
    "\\right)\n",
    "\\, dx\\, dy\n",
    "=\n",
    "H(Y) - H(Y \\mid X)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
